{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f3b12e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/Commonists/pageview-api.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6809818",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee0199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only for the first time #\n",
    "import csv\n",
    "f = open('WikiDataSet.csv', 'w', newline = '', encoding=\"utf-8\") #open the csv at the first time \n",
    "writer = csv.writer(f)\n",
    "col_list = ['Page Title', 'URL', 'Num Of Pictures', 'Num Of Languages', 'Num Of References', 'Num Of Contents','Num Of Tables', 'Num Of Categories', 'Num Of Redirects', 'Views In 30 Last Days', 'Avg Views Per Day', 'Total Views', 'Num Of Edits']\n",
    "writer.writerow(col_list)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8960cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "url = \"http://en.wikipedia.org/wiki/Special:Random\" #Random page picker from Wikipedia\n",
    "workers_num = 16 #Parallel processors\n",
    "requests_per_time = 1 #Requests per second\n",
    "time = 1 #second\n",
    "url_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7fcab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import threading #Run all processors simultaneously\n",
    "from queue import Queue #Processor Queue\n",
    "from threading import Semaphore, Timer #Time and semaphor to approve or restrict CPU access\n",
    "import html #html request\n",
    "import sys\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import wikipedia\n",
    "import pageviewapi\n",
    "import pageviewapi.period\n",
    "\n",
    "\n",
    "#set parameters\n",
    "workers_num = int(workers_num) #Number of processors at the same time\n",
    "requests_per_time = int(requests_per_time) #Requests per second\n",
    "time = int(time)#second\n",
    "\n",
    "link = url #Random address\n",
    "\n",
    "#semaphore for sync printing\n",
    "print_semaphore = Semaphore(1)\n",
    "\n",
    "#producer(scheduler) architecture\n",
    "class Producer:\n",
    "    print(\"IN Producer\")\n",
    "    def __init__(self, queue):\n",
    "        self.queue = queue\n",
    "    def run(self):\n",
    "        threading.Timer(time, self.run).start()\n",
    "        # print(\"Reset\")\n",
    "        for i in range(requests_per_time):\n",
    "            self.queue.put(i)\n",
    "            \n",
    "#consumers(workers) architecture\n",
    "class Consumer:\n",
    "    print(\"IN CONSUMER\")\n",
    "    def __init__(self, queue, id):\n",
    "        self.queue = queue\n",
    "        self.__id = id\n",
    "    def run(self):\n",
    "        #while True:\n",
    "        for i in range(100):\n",
    "            try:\n",
    "                    #check if has permission to send a request (based on requests_per_time) if yes continue, else blocked until producer initiates new permissions\n",
    "                self.queue.get()\n",
    "                    #scrape and search keywords\n",
    "                    #sync printing\n",
    "                print_semaphore.acquire()\n",
    "                url = Scraper.wiki_scrape(link)\n",
    "                print(\"Worker: \"+str(self.__id)) #Printing which processor it runs on\n",
    "                print(\"Random URL: \"+ str(url)) #Printing the URL\n",
    "                print(\"-----------------------------\")\n",
    "                print_semaphore.release()\n",
    "                self.queue.task_done()\n",
    "            except KeyboardInterrupt:\n",
    "                print('Interrupted')\n",
    "                sys.exit()\n",
    "\n",
    "#scrape a random wiki page\n",
    "class Scraper:\n",
    "    def wiki_scrape(url): \n",
    "        print(\"in wiki\")\n",
    "        url_list = []\n",
    "        res = requests.get(url,)\n",
    "        res.raise_for_status() #Checking for an error during the process\n",
    "        wiki = BeautifulSoup(res.text,\"html.parser\")\n",
    "        print(\"URL: \", res.url)           \n",
    "        \n",
    "        title = wiki.find(\"h1\" , attrs = {\"class\":\"firstHeading\"})#Page Title\n",
    "        url_list.append(title.text)\n",
    "        \n",
    "        url_list.append(str(res.url))#URL\n",
    "        \n",
    "        pic_counter = wiki.find_all(\"a\" , attrs = {\"class\":\"image\"})#Num Of Pictures\n",
    "        url_list.append(len(pic_counter))\n",
    "        \n",
    "        languages_counter = wiki.find_all(\"a\" , attrs = {\"class\":\"interlanguage-link-target\"})#Num Of Languages\n",
    "        url_list.append(len(languages_counter))\n",
    "        \n",
    "        references_counter = wiki.find_all(\"span\" , attrs = {\"class\":\"reference-text\"})#Num Of References\n",
    "        url_list.append(len(references_counter))\n",
    "        \n",
    "        contents_counter = wiki.find_all(\"li\" , attrs = {\"class\":\"toclevel-1\"})#Num Of Contents\n",
    "        url_list.append(len(contents_counter))\n",
    "        \n",
    "        tables_counter = wiki.find_all(\"table\" , attrs = {\"class\":\"wikitable\"})#Num Of Tables\n",
    "        url_list.append(len(tables_counter))\n",
    "        \n",
    "        categories_counter = wiki.find(\"div\" , attrs = {\"class\":\"mw-normal-catlinks\"}).find_all('li') #Num Of Categories\n",
    "        url_list.append(len(categories_counter))\n",
    "\n",
    "        info_url = (wiki(\"ul\" , attrs = {\"class\":\"vector-menu-content-list\"})[7].find_all('li'))[5].find('a').get('href')#page title\n",
    "        html = 'https://en.wikipedia.org' + info_url #make the link\n",
    "        html = requests.get(html)\n",
    "        info_page = BeautifulSoup(html.text, \"html.parser\")#html\n",
    "        \n",
    "        Num_of_redirects_to_the_page = ((info_page(\"table\" , attrs = {\"class\":\"wikitable mw-page-info\"})[0].find_all('tr'))[8].find_all('td'))[1]#Num Of Redirects\n",
    "        url_list.append(Num_of_redirects_to_the_page.text)\n",
    "        \n",
    "        views_in_last_30 = info_page.find(\"div\" , attrs = {\"class\":\"mw-pvi-month\"})#Views In 30 Last Days\n",
    "        url_list.append(views_in_last_30.text)\n",
    "        \n",
    "        ##with API library (the last if from 7.1.2015 mm.dd.yy)##\n",
    "        Avg_views_per_day = pageviewapi.period.sum_last('en.wikipedia', title.text, last=2373, access='all-access', agent='all-agents') / 2373 # Avg Views Per Day\n",
    "        url_list.append(Avg_views_per_day)\n",
    "        \n",
    "        total_views = pageviewapi.period.sum_last('en.wikipedia', title.text, last=2373, access='all-access', agent='all-agents') #Total Views\n",
    "        url_list.append(total_views)\n",
    "        \n",
    "        edits = ((info_page(\"table\" , attrs = {\"class\":\"wikitable mw-page-info\"})[2].find_all('tr'))[4].find_all('td'))[1]#Num Of Edits\n",
    "        url_list.append(edits.text)\n",
    "        \n",
    "        #Adds variables that will enter the list and print on the same line\n",
    "        Scraper.csv_maker(url_list)\n",
    "        \n",
    "        return res.url\n",
    "    \n",
    "    def csv_maker(urls):\n",
    "        print(\"in csv\")\n",
    "        f = open('WikiDataSet.csv', 'a', newline = '', encoding=\"utf-8\") #Create the file\n",
    "        writer = csv.writer(f)       \n",
    "        writer.writerow(urls) \n",
    "        print(\"PRINTED\")\n",
    "\n",
    "        f.close() \n",
    "\n",
    "    \n",
    "#create, start and join consumers(workers) and producer(scheduler) threads\n",
    "def main():\n",
    "    q = Queue(maxsize=requests_per_time) #Scheduled Queue\n",
    "        #create and start producer thread\n",
    "    scheduler = Producer(q)\n",
    "    scheduler_thread = threading.Thread(target=scheduler.run)\n",
    "    scheduler_thread.start() #Create and start scheduled threading\n",
    "        #create and start consumers thread\n",
    "    workers = [] \n",
    "    for worker_id in range(workers_num): #For each processor\n",
    "        worker = Consumer(q, worker_id)\n",
    "        workers.append(worker) #Add processor to processor list\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        t = threading.Thread(target=worker.run)\n",
    "        t.start()\n",
    "        worker_threads.append(t)#Connect all threads and finish\n",
    "        #join all threads and terminate\n",
    "    scheduler_thread.join()\n",
    "    for t in worker_threads:\n",
    "        t.join()      \n",
    "    print(\"DONE!\")\n",
    "      \n",
    "main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
